{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, \\\n",
    "    f1_score, roc_auc_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "   true  pred\n0     1     0\n1     1     1\n2     1     1\n3     0     0\n4     1     1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>true</th>\n      <th>pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"datasets/classification.csv\")\n",
    "dataset.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 34 59 64\n"
     ]
    }
   ],
   "source": [
    "TP = sum([1 for row in dataset.values if row[0] == 1 and row[1] == 1])\n",
    "TN = sum([1 for row in dataset.values if row[0] == 0 and row[1] == 0])\n",
    "FP = sum([1 for row in dataset.values if row[0] == 0 and row[1] == 1])\n",
    "FN = sum([1 for row in dataset.values if row[0] == 1 and row[1] == 0])\n",
    "# tn, fp, fn, tp = confusion_matrix(dataset.true, dataset.pred).ravel()\n",
    "print(TP, FP, FN, TN, sep=\" \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54 0.56 0.42 0.48\n"
     ]
    }
   ],
   "source": [
    "ac = round(accuracy_score(dataset.true, dataset.pred), 2)\n",
    "pr = round(precision_score(dataset.true, dataset.pred), 2)\n",
    "rc = round(recall_score(dataset.true, dataset.pred), 2)\n",
    "f = round(f1_score(dataset.true, dataset.pred), 2)\n",
    "print(ac, pr, rc, f, sep=\" \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "   true  score_logreg  score_svm  score_knn  score_tree\n0     0      0.683832   0.145976   0.787063    0.500000\n1     1      0.801966   0.239511   1.000000    0.833333\n2     0      0.382315  -0.245701   0.000000    0.000000\n3     1      0.506797  -0.137058   0.000000    0.105263\n4     1      0.488781  -0.154148   0.000000    0.105263",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>true</th>\n      <th>score_logreg</th>\n      <th>score_svm</th>\n      <th>score_knn</th>\n      <th>score_tree</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.683832</td>\n      <td>0.145976</td>\n      <td>0.787063</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.801966</td>\n      <td>0.239511</td>\n      <td>1.000000</td>\n      <td>0.833333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0.382315</td>\n      <td>-0.245701</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0.506797</td>\n      <td>-0.137058</td>\n      <td>0.000000</td>\n      <td>0.105263</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0.488781</td>\n      <td>-0.154148</td>\n      <td>0.000000</td>\n      <td>0.105263</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_ = pd.read_csv(\"datasets/scores.csv\")\n",
    "scores_.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "1.0\n",
      "score_logreg\n",
      "0.72\n",
      "score_svm\n",
      "0.71\n",
      "score_knn\n",
      "0.64\n",
      "score_tree\n",
      "0.69\n"
     ]
    }
   ],
   "source": [
    "for label, column in scores_.items():\n",
    "    print(label)\n",
    "    score = roc_auc_score(scores_.true, column)\n",
    "    print(round(score, 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  true\n",
      "Max precision: 1.0 with recall: 1.0\n",
      "Label:  score_logreg\n",
      "Max precision: 0.63 with recall: 0.77\n",
      "Label:  score_svm\n",
      "Max precision: 0.62 with recall: 0.72\n",
      "Label:  score_knn\n",
      "Max precision: 0.61 with recall: 0.76\n",
      "Label:  score_tree\n",
      "Max precision: 0.65 with recall: 0.74\n"
     ]
    }
   ],
   "source": [
    "for label, column in scores_.items():\n",
    "    print(\"Label: \", label)\n",
    "    pr, rec, th  = precision_recall_curve(scores_.true, column)\n",
    "    max_p = 0\n",
    "    rec_r = 0\n",
    "    for p, r in zip(pr, rec):\n",
    "        if r >= 0.7 and p > max_p:\n",
    "            max_p = p\n",
    "            rec_r = r\n",
    "    print(f\"Max precision: {round(max_p, 2)} with recall: {round(rec_r, 2)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}